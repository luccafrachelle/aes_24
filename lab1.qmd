---
title: "Practico 1"
format: html
date: "2024-09-16"
echo: false
---
## Ejemplo clase

```{r}
#| echo: false
#| output: false
library(tidyverse)
library(tidymodels)
library(schrute) 
library(vip)
library(knitr)
set.seed(1234)
```


```{r}
office_info <- theoffice %>% 
    select(season, episode_name, director, writer,
character, text, imdb_rating)
office_info %>% head() %>% kable()
```



```{r}
characters <- office_info %>% 
  count(episode_name, character) %>% 
  add_count(character, wt = n, name = "character_count")  %>%  
  filter(character_count > 800) %>% 
  select(-character_count)  %>% 
  pivot_wider(
    names_from = character,
    values_from = n,
    values_fill = list(n = 0)) 
```

```{r}
creators <- office_info |>
  distinct(episode_name, director, writer) |>
  pivot_longer(director:writer, names_to = "role", values_to = "person") |>
  separate_rows(person, sep = ";") |>
  add_count(person) |>
  mutate(person = case_when(
    n <= 10 ~ 'Guest',
    n > 10 ~ person
  )) |>
  distinct(episode_name, person) |>
  mutate(person_value = 1) |>
  pivot_wider(
    names_from = person,
    values_from = person_value,
    values_fill = list(person_value = 0))
```

```{r}
#| output: false
office <- office_info |> 
  distinct(season, episode_name, imdb_rating) |>
  inner_join(characters) |>
  inner_join(creators) |>
  mutate_at("season", as.factor)
```

```{r}
office |>
  ggplot(aes(season, imdb_rating, fill = as.factor(season))) +
  geom_boxplot(show.legend = FALSE)  +
  labs(title = "Rating por temporada", x = "Temporada", y = "Rating")
```

```{r}
office_split <- initial_split(office, 
                              strata = season,
                              prop = 3/4) 
office_train <- training(office_split)
office_test <- testing(office_split)
```

```{r}
office_rec <- recipe(imdb_rating ~ ., data = office_train) |>
  update_role(episode_name, new_role = "ID") |> step_dummy(season) |> 
  step_normalize(all_numeric(), -all_outcomes()) 
```

```{r}
tune_spec <- linear_reg(penalty = tune(), mixture = 1) |> #Con tune() indicamos parámetro a ser ajustado.
  set_engine("glmnet") 
```

```{r}
tune_wf <- workflow() |>
  add_recipe(office_rec) |>
  add_model(tune_spec)
```

```{r}
office_cv <- vfold_cv(office_train, v = 5) 
lambda_grid <- grid_regular(penalty(c(-10,-1)),  levels = 50) 
lasso_grid <- tune_grid(
  tune_wf,
  resamples = office_cv,
  grid = lambda_grid
)
```


```{r}
lasso_grid |>
  collect_metrics() |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(linewidth = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none") + labs(title = "Lasso")
```

```{r}
office_boot <- bootstraps(office_train, strata = season)
lambda_grid <- grid_regular(penalty(c(-10,-1)), levels = 50) #Definimos la grilla

lasso_grid <- tune_grid(
  tune_wf,
  resamples = office_boot,
  grid = lambda_grid
)
```

```{r}
lowest_rmse <- lasso_grid |>
  select_best(metric = "rmse")

final_lasso <- finalize_workflow(tune_wf, lowest_rmse)
```

```{r}
final_lasso |>
  fit(office_train) |>
  extract_fit_engine() |>
  vi(lambda = lowest_rmse$penalty) |> #Es muy importante marcar el lambda!
  ggplot(aes(x = Importance, y = reorder(Variable, Importance), fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(title = "Importancia de variables en Lasso", x = "Importancia", y = "Variable")
```
```{r}
library(ISLR2)
data(College)
df = College
```

### Estructura de los datos

```{r}
df %>% head() %>% kable()
```
```{r}
df_split = initial_split(df, prop = 0.8)
df_train = training(df_split)
df_test = testing(df_split)
```
## Least Squares
```{r}
lm_mod = linear_reg() %>%
  set_engine("lm") 
```

```{r}
lm_fit = lm_mod %>% fit(Apps ~ ., data = df_train)
```



```{r}
least_squares = bind_rows(
  lm_fit %>% predict(df_train) %>% bind_cols(df_train) %>% metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "train"),
  lm_fit %>% predict(df_test) %>% bind_cols(df_test) %>% metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "test")) %>% 
  pivot_wider(names_from = data, values_from = .estimate)
```

  
### Importancia de variables
```{r}
lm_fit %>% 
  extract_fit_engine()  %>% 
  vip(num_features = 10 , geom = "col", aesthetics = list(fill = "skyblue" , color = "black"))
```

### Errores
```{r}
least_squares %>% kable()
```
## Ridge con penanlidad 0 

```{r}
df_rec <- recipe(Apps ~ ., data = df_train) |>
   step_dummy(Private) |> 
  step_normalize(all_numeric(), -all_outcomes()) 
```

```{r}
tune_spec <- linear_reg(penalty = 0, mixture = 0)  %>%  
  set_engine("lm") 
```

```{r}
tune_wf <- workflow() |>
  add_recipe(df_rec) |>
  add_model(tune_spec)
```

```{r}
ridge_fit <- tune_wf %>% fit(df_train)
```


```{r}
ridge = bind_rows(
  ridge_fit %>% predict(df_train) %>% bind_cols(df_train) %>% metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "train"),
  ridge_fit %>% predict(df_test) %>% bind_cols(df_test) %>% metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "test")) %>% 
  pivot_wider(names_from = data, values_from = .estimate)
```
### Importancia de variables

```{r}
ridge_fit %>% 
  extract_fit_engine()  %>% 
  vip(num_features = 10 , geom = "col", aesthetics = list(fill = "skyblue" , color = "black"))
```

### Errores
```{r}
ridge  %>% kable()
```


## Ridge

```{r}
df_rec <- recipe(Apps ~ ., data = df_train) |>
   step_dummy(Private) |> 
  step_normalize(all_numeric(), -all_outcomes()) 
```

```{r}
tune_spec <- linear_reg(penalty = tune(), mixture = 0)  %>%  
  set_engine("glmnet") 
```

```{r}
tune_wf <- workflow() |>
  add_recipe(df_rec) |>
  add_model(tune_spec)
```

```{r}
df_cv <- vfold_cv(df_train, v = 5) 
ridge_grid <- grid_regular(penalty(c(-5,3) ),  levels = 100) 
ridge_grid <- tune_grid(
  tune_wf,
  resamples = df_cv,
  grid = ridge_grid)
```

```{r}
ridge_grid |>
  collect_metrics() |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(linewidth = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```
### Lambda Optimo
```{r}
ridge_grid %>% select_best(metric = "rmse") %>% kable()
```

```{r}
ridge_final <- finalize_workflow(tune_wf, ridge_grid %>% select_best(metric = "rmse"))
```

### Importancia de variables

```{r}
ridge_final %>% 
  fit(df_train) %>% 
  extract_fit_engine()  %>% 
  vip(num_features = 10 , geom = "col", aesthetics = list(fill = "skyblue" , color = "black"))
```

```{r}
ridge = bind_rows(
  ridge_final %>% fit(df_train) %>% predict(df_train) %>% bind_cols(df_train) %>% 
  metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "train"),
  ridge_final %>% fit(df_train) %>% predict(df_test) %>% bind_cols(df_test) %>% 
  metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "test")) %>% 
  pivot_wider(names_from = data, values_from = .estimate)
```
### Errores
```{r}
ridge %>% kable()
```

## Lasso
```{r}
tune_spec <- linear_reg(penalty = tune(), mixture = 1)  %>%  
  set_engine("glmnet") 
```

```{r}
tune_wf <- workflow() |>
  add_recipe(df_rec) |>
  add_model(tune_spec)
```

```{r}
df_cv <- vfold_cv(df_train, v = 5)
lambda_grid <- grid_regular(penalty(c(-5 , 3) ),  levels = 100)
lasso_grid <- tune_grid(
  tune_wf,
  resamples = df_cv,
  grid = lambda_grid)
```


```{r}
lasso_grid |>
  collect_metrics() |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(linewidth = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```
### Lambda Optimo
```{r}
lasso_grid %>% select_best(metric = "rmse") %>% kable()
```
### Autoplot de importancia variables
```{r}
lasso_grid %>% autoplot()
```

```{r}
lasso_final <- finalize_workflow(tune_wf, lasso_grid %>% select_best(metric = "rmse"))
```

### Importancia de variables

```{r}
lasso_final %>% 
  fit(df_train) %>% 
  extract_fit_engine()  %>% 
  vip(num_features = 10 , geom = "col", aesthetics = list(fill = "skyblue" , color = "black"))
```


```{r}
lasso = bind_rows(
  lasso_final %>% fit(df_train) %>% predict(df_train) %>% bind_cols(df_train) %>% 
  metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "train"),
  lasso_final %>% fit(df_train) %>% predict(df_test) %>% bind_cols(df_test) %>% 
  metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "test")) %>% 
  pivot_wider(names_from = data, values_from = .estimate)
```

### Error 
```{r}
lasso %>% kable()
```

## KNN 
```{r}
knn_spec <- nearest_neighbor(weight_func = "rectangular", neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")
```

```{r}
knn_rec <- recipe(Apps ~ ., data = df_train) |>
  step_dummy(Private) |>
  step_normalize(all_numeric(), -all_outcomes())
```

```{r}
tune_wf <- workflow() |>
  add_recipe(knn_rec) |>
  add_model(knn_spec)
```

```{r}
nearest_neighbor_grid <- grid_regular(
  neighbors(range = c(1, 25)), 
  levels = 25
)
```

```{r}
knn_cv <- vfold_cv(df_train, v = 5)
knn_grid <- tune_grid(
  tune_wf,
  resamples = knn_cv,
  grid = nearest_neighbor_grid
)
```

```{r}
knn_grid |>
  collect_metrics() |>
  ggplot(aes(neighbors, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(linewidth = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  theme(legend.position = "none")
```
### Cantidad optima de vecinos
```{r}
knn_grid %>% select_best(metric = "rmse") %>% kable(caption = "Cantidad oprima de vecinos")
```

```{r}
knn_final <- finalize_workflow(tune_wf, knn_grid %>% select_best(metric = "rmse"))
```

```{r}
 knn = bind_rows(
  knn_final %>% fit(df_train) %>% predict(df_train) %>% bind_cols(df_train) %>% 
  metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "train"),
  knn_final %>% fit(df_train) %>% predict(df_test) %>% bind_cols(df_test) %>% 
  metrics(truth = Apps, estimate = .pred) %>% mutate(.estimate = round(.estimate, 2)) %>% mutate(data = "test")) %>% 
  pivot_wider(names_from = data, values_from = .estimate)
```

### Error de testeo
```{r}
knn %>% kable()
```



## Comparación de modelos

```{r}
bind_rows(
  least_squares %>% mutate(model = "Least Squares"),
  ridge %>% mutate(model = "Ridge"),
  lasso %>% mutate(model = "Lasso"),
  knn %>% mutate(model = "KNN")
) %>% ggplot(aes(x = fct_reorder(model, test , .desc = TRUE), y = test , fill = model)) +
  geom_col(position = "dodge") +
  labs(title = "Comparación de modelos", x = "Modelo", y = "RMSE") + 
  coord_flip() 
```

```{r}
```

```{r}


```{r}

```

```{r}
```






